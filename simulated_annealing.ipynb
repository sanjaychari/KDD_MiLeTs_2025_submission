{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67360a5f",
   "metadata": {},
   "source": [
    "# Simulated Annealing Imputation for Time Series Gap with Bidirectional Weekly Seasonality and Stochastic Noise\n",
    "\n",
    "This notebook fills a missing gap in time series data from **October 1, 2024** to **January 31, 2025** at **10‑minute** intervals (preserving the original seconds offset). It uses a bidirectional weekly seasonal profile (from 28 days before and after the gap) blended across the gap, adds a stochastic noise component, and formulates a QUBO to softly enforce trend, seasonality, and smoothness. The QUBO is solved via simulated annealing.\n",
    "\n",
    "## Steps\n",
    "1. Setup & Imports\n",
    "2. Load Data & Define Gap\n",
    "3. Bidirectional Seasonal Profile (mean & std)\n",
    "4. Build Expected Values (trend + seasonality + noise)\n",
    "5. QUBO Construction & Annealing\n",
    "6. Integrate & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c2a1bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annealer available: True\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup & Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import random, math\n",
    "\n",
    "# Try D-Wave's neal\n",
    "try:\n",
    "    from neal import SimulatedAnnealingSampler\n",
    "    annealer_available = True\n",
    "except ImportError:\n",
    "    annealer_available = False\n",
    "    print('neal not available; using custom SA')\n",
    "\n",
    "print('Annealer available:', annealer_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc08ba1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred interval: 0 days 00:10:00\n",
      "Gap from 2024-10-01 00:00:07 to 2025-01-30 23:50:07, 17568 points\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Data & Define Gap\n",
    "csv_file = 'router_metrics_timeseries_patchtst_expsmooth.csv'\n",
    "df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Boundary timestamps\n",
    "Y_start_date = df.index[df.index < '2024-10-01'].max()\n",
    "Y_end_date   = df.index[df.index > '2025-01-31'].min()\n",
    "\n",
    "# Infer sampling interval\n",
    "pre_idx = df.index[df.index < Y_start_date]\n",
    "interval = pre_idx[-1] - pre_idx[-2]\n",
    "print(f'Inferred interval: {interval}')\n",
    "\n",
    "# Adjusted gap boundaries\n",
    "adjusted_gap_start = Y_start_date + interval\n",
    "adjusted_gap_end   = Y_end_date   - interval\n",
    "missing_dates = pd.date_range(start=adjusted_gap_start, end=adjusted_gap_end, freq=interval)\n",
    "N = len(missing_dates)\n",
    "print(f'Gap from {adjusted_gap_start} to {adjusted_gap_end}, {N} points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90efe033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed weekly profiles and stds\n"
     ]
    }
   ],
   "source": [
    "# 3. Bidirectional Seasonal Profile (mean & std)\n",
    "pre_window_start  = Y_start_date - timedelta(days=28)\n",
    "pre_window_end    = Y_start_date\n",
    "post_window_start = Y_end_date\n",
    "post_window_end   = Y_end_date + timedelta(days=28)\n",
    "\n",
    "pre_seg  = df.loc[pre_window_start:pre_window_end]\n",
    "post_seg = df.loc[post_window_start:post_window_end]\n",
    "\n",
    "# Compute linear trend parameters\n",
    "total_steps = N + 1\n",
    "trend_info = {}\n",
    "for col in df.columns:\n",
    "    y0 = df.loc[Y_start_date, col]\n",
    "    y1 = df.loc[Y_end_date, col]\n",
    "    d_step = (y1 - y0) / total_steps\n",
    "    trend_info[col] = {'y0': y0, 'y1': y1, 'd': d_step}\n",
    "\n",
    "sec_per_week = 7 * 24 * 3600\n",
    "res_pre  = pd.DataFrame(index=pre_seg.index,  columns=df.columns)\n",
    "res_post = pd.DataFrame(index=post_seg.index, columns=df.columns)\n",
    "\n",
    "for col in df.columns:\n",
    "    info = trend_info[col]\n",
    "    steps_pre  = ((pre_seg.index - Y_start_date) / pd.Timedelta(interval)).astype(int)\n",
    "    lin_pre    = info['y0'] + steps_pre * info['d']\n",
    "    res_pre[col]  = pre_seg[col] - lin_pre\n",
    "\n",
    "    steps_post = ((post_seg.index - Y_start_date) / pd.Timedelta(interval)).astype(int)\n",
    "    lin_post   = info['y0'] + steps_post * info['d']\n",
    "    res_post[col] = post_seg[col] - lin_post\n",
    "\n",
    "tow_pre  = ((res_pre.index.view(np.int64)  // 1_000_000_000) % sec_per_week)\n",
    "tow_post = ((res_post.index.view(np.int64) // 1_000_000_000) % sec_per_week)\n",
    "res_pre['_tow']  = tow_pre\n",
    "res_post['_tow'] = tow_post\n",
    "\n",
    "weekly_profile_pre  = res_pre.groupby('_tow').mean()\n",
    "weekly_profile_post = res_post.groupby('_tow').mean()\n",
    "weekly_std_pre      = res_pre.groupby('_tow').std()\n",
    "weekly_std_post     = res_post.groupby('_tow').std()\n",
    "\n",
    "print('Computed weekly profiles and stds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea865d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built gap_imputed with non‑negative, reduced stochastic component (β= 0.4 )\n"
     ]
    }
   ],
   "source": [
    "# 4. Build Expected Values: Trend + Seasonality + Reduced Noise + Non‑negative clamp\n",
    "np.random.seed(42)\n",
    "beta = 0.4   # reduce noise amplitude\n",
    "\n",
    "gap_imputed = pd.DataFrame(index=missing_dates, columns=df.columns)\n",
    "\n",
    "for col in df.columns:\n",
    "    info   = trend_info[col]\n",
    "    y0     = info['y0']\n",
    "    d_step = info['d']\n",
    "    exp_vals = []\n",
    "    for i, ts in enumerate(missing_dates):\n",
    "        # linear trend\n",
    "        lin = y0 + (i+1)*d_step\n",
    "\n",
    "        # seasonal components (bidirectional as before)…\n",
    "        key = int((ts.value // 1_000_000_000) % sec_per_week)\n",
    "        # find nearest keys in pre/post profiles…\n",
    "        val_pre, std_pre = weekly_profile_pre[col].get(key, 0), weekly_std_pre[col].get(key, 0)\n",
    "        val_post, std_post = weekly_profile_post[col].get(key, 0), weekly_std_post[col].get(key, 0)\n",
    "\n",
    "        alpha = (i+1)/(N+1)\n",
    "        seas = (1-alpha)*val_pre + alpha*val_post\n",
    "        blended_std = (1-alpha)*std_pre + alpha*std_post\n",
    "\n",
    "        # stochastic noise (scaled)\n",
    "        noise = beta * np.random.normal(0, blended_std)\n",
    "\n",
    "        raw = lin + seas + noise\n",
    "        # clamp negatives to zero\n",
    "        val = max(0, raw)\n",
    "        exp_vals.append(val)\n",
    "\n",
    "    gap_imputed[col] = exp_vals\n",
    "\n",
    "print(\"Built gap_imputed with non‑negative, reduced stochastic component (β=\", beta, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfb31a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: ifInMulticastPkts\n",
      "Done ifInMulticastPkts\n",
      "Processing column: ifInBroadcastPkts\n",
      "Done ifInBroadcastPkts\n",
      "Processing column: ifOutMulticastPkts\n",
      "Done ifOutMulticastPkts\n",
      "Processing column: ifOutBroadcastPkts\n",
      "Done ifOutBroadcastPkts\n",
      "Processing column: ifHCInOctets\n",
      "Done ifHCInOctets\n",
      "Processing column: ifHCInUcastPkts\n",
      "Done ifHCInUcastPkts\n",
      "Processing column: ifHCInMulticastPkts\n",
      "Done ifHCInMulticastPkts\n",
      "Processing column: ifHCInBroadcastPkts\n",
      "Done ifHCInBroadcastPkts\n",
      "Processing column: ifHCOutOctets\n",
      "Done ifHCOutOctets\n",
      "Processing column: ifHCOutUcastPkts\n",
      "Done ifHCOutUcastPkts\n",
      "Processing column: ifHCOutMulticastPkts\n",
      "Done ifHCOutMulticastPkts\n",
      "Processing column: ifHCOutBroadcastPkts\n",
      "Done ifHCOutBroadcastPkts\n",
      "Processing column: ifHighSpeed\n",
      "Done ifHighSpeed\n",
      "QUBO imputation complete; imputed_df shape: (17568, 13)\n"
     ]
    }
   ],
   "source": [
    "# 5. QUBO Construction & Simulated Annealing with stronger high‐value penalty\n",
    "P = 1e6\n",
    "gamma_high = 1e6   # increased weight for values above max\n",
    "gamma_low  = 1e4\n",
    "imputed_columns = {}\n",
    "Np = len(missing_dates)\n",
    "\n",
    "def add_qubo_bias(Q, i, j, bias):\n",
    "    key = (min(i, j), max(i, j))\n",
    "    Q[key] = Q.get(key, 0) + bias\n",
    "\n",
    "for col in df.columns:\n",
    "    print(f\"Processing column: {col}\")\n",
    "    Q = {}\n",
    "    var_idx = {}\n",
    "    idx_var = {}\n",
    "    v = 0\n",
    "    expv = gap_imputed[col].values\n",
    "    cand = []\n",
    "\n",
    "    # Discretize each expected continuous value into integer candidates\n",
    "    for i, e in enumerate(expv):\n",
    "        opts = {np.floor(e), np.ceil(e)}\n",
    "        if len(opts) == 1:\n",
    "            x = next(iter(opts))\n",
    "            opts |= {x - 1, x + 1}\n",
    "        opts = sorted(x for x in opts if x >= 0)\n",
    "        cand.append(opts)\n",
    "        for x in opts:\n",
    "            var_idx[(i, x)] = v\n",
    "            idx_var[v] = (i, x)\n",
    "            v += 1\n",
    "\n",
    "    # Historical bounds\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "\n",
    "    # (a) Trend deviation & high‐/low‐value penalties\n",
    "    for (i, x), qi in var_idx.items():\n",
    "        # deviation from expected\n",
    "        add_qubo_bias(Q, qi, qi, (x - expv[i])**2)\n",
    "        # stronger penalty if above historical max: quartic penalty\n",
    "        if x > max_val:\n",
    "            add_qubo_bias(Q, qi, qi, gamma_high * (x - max_val)**4)\n",
    "        # penalty if below historical min\n",
    "        if x < min_val:\n",
    "            add_qubo_bias(Q, qi, qi, gamma_low * (min_val - x)**2)\n",
    "\n",
    "    # (b) Boundary conditions\n",
    "    info = trend_info[col]\n",
    "    y0, y1, d_step = info['y0'], info['y1'], info['d']\n",
    "    for x in cand[0]:\n",
    "        add_qubo_bias(Q, var_idx[(0, x)], var_idx[(0, x)], (x - y0 - d_step)**2)\n",
    "    for x in cand[-1]:\n",
    "        add_qubo_bias(Q, var_idx[(Np - 1, x)], var_idx[(Np - 1, x)], (y1 - x - d_step)**2)\n",
    "\n",
    "    # (c) Smoothness between consecutive points\n",
    "    for i in range(Np - 1):\n",
    "        for xi in cand[i]:\n",
    "            vi = var_idx[(i, xi)]\n",
    "            for xj in cand[i + 1]:\n",
    "                vj = var_idx[(i + 1, xj)]\n",
    "                add_qubo_bias(Q, vi, vj, ((xj - xi) - d_step)**2)\n",
    "\n",
    "    # (d) One‐hot constraints per time‐slot\n",
    "    for i, opts in enumerate(cand):\n",
    "        for a in range(len(opts)):\n",
    "            va = var_idx[(i, opts[a])]\n",
    "            add_qubo_bias(Q, va, va, -2 * P)\n",
    "            for b in range(a + 1, len(opts)):\n",
    "                vb = var_idx[(i, opts[b])]\n",
    "                add_qubo_bias(Q, va, vb, 2 * P)\n",
    "\n",
    "    # Solve the QUBO\n",
    "    if annealer_available:\n",
    "        sampler = SimulatedAnnealingSampler()\n",
    "        sol = sampler.sample_qubo(Q, num_reads=50).first.sample\n",
    "    else:\n",
    "        sol = current_solution  # your custom SA fallback\n",
    "\n",
    "    # Extract the chosen values, fallback‐rounding any unassigned slots\n",
    "    imputed_vals = [None] * Np\n",
    "    for vid, bit in sol.items():\n",
    "        if bit == 1:\n",
    "            idx, val = idx_var[vid]\n",
    "            imputed_vals[idx] = val\n",
    "    for i in range(Np):\n",
    "        if imputed_vals[i] is None:\n",
    "            imputed_vals[i] = int(round(expv[i]))\n",
    "\n",
    "    imputed_columns[col] = imputed_vals\n",
    "    print(f\"Done {col}\")\n",
    "\n",
    "imputed_df = pd.DataFrame(imputed_columns, index=missing_dates)\n",
    "print(\"QUBO imputation complete; imputed_df shape:\", imputed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f85827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Integrate & Save\n",
    "df.drop(df.loc[adjusted_gap_start:adjusted_gap_end].index, inplace=True)\n",
    "df_filled = pd.concat([df, imputed_df])\n",
    "df_filled.sort_index(inplace=True)\n",
    "df_filled.index.name = \"date\"\n",
    "\n",
    "print(df_filled.loc[Y_start_date - interval : Y_end_date + interval])\n",
    "df_filled.to_csv('router_metrics_Simulated_annealing.csv')\n",
    "print('Saved filled CSV.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2022547-9f24-40eb-bdfc-b8f962e8869f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
