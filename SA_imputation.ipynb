{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179cb3e7",
   "metadata": {},
   "source": [
    "# Time Series Imputation Benchmark\n",
    "\n",
    "Compare **simulated Annealing** to SOTA methods (BRITS, SAITS, KNN, MICE) on standard datasets loaded via **TSDB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90a24e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imports\n",
    "import tsdb\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neal import SimulatedAnnealingSampler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pypots.imputation import SAITS, BRITS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603a78f",
   "metadata": {},
   "source": [
    "## 3. Load datasets via TSDB\n",
    "\n",
    "We load four benchmarks: Air Quality (Italy), Electricity Load, PeMS Traffic, PhysioNet 2012 ICU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e04b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 10:06:02 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality\n",
      "2025-05-22 10:06:02 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...\n",
      "2025-05-22 10:06:02 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...\n",
      "2025-05-22 10:06:02 [INFO]: Loaded successfully!\n",
      "2025-05-22 10:06:02 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams\n",
      "2025-05-22 10:06:02 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...\n",
      "2025-05-22 10:06:02 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...\n",
      "2025-05-22 10:06:02 [INFO]: Loaded successfully!\n",
      "2025-05-22 10:06:02 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic\n",
      "2025-05-22 10:06:02 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...\n",
      "2025-05-22 10:06:02 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...\n",
      "2025-05-22 10:06:02 [INFO]: Loaded successfully!\n",
      "2025-05-22 10:06:02 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2025-05-22 10:06:02 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2025-05-22 10:06:02 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2025-05-22 10:06:02 [INFO]: Loaded successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AirQuality raw keys: ['X']\n",
      "  → Loaded AirQuality as DataFrame (9357, 15)\n",
      "\n",
      "Electricity raw keys: ['X']\n",
      "  → Loaded Electricity as DataFrame (140256, 370)\n",
      "\n",
      "Traffic raw keys: ['X']\n",
      "  → Loaded Traffic as DataFrame (17544, 863)\n",
      "\n",
      "PhysioNet raw keys: ['set-a', 'set-b', 'set-c', 'outcomes-a', 'outcomes-b', 'outcomes-c', 'static_features']\n",
      "  → Loaded PhysioNet as DataFrame (180552, 43)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Load datasets via TSDB (robust to missing 'X' key)\n",
    "# map our friendly names to TSDB dataset keys\n",
    "dataset_map = {\n",
    "    \"AirQuality\": \"italy_air_quality\",\n",
    "    \"Electricity\": \"electricity_load_diagrams\",\n",
    "    \"Traffic\": \"pems_traffic\",\n",
    "    \"PhysioNet\": \"physionet_2012\"\n",
    "}\n",
    "\n",
    "data_dict = {}\n",
    "for display_name, tsdb_name in dataset_map.items():\n",
    "    raw = tsdb.load(tsdb_name, use_cache=True)\n",
    "    # Debug print to see what keys we got:\n",
    "    print(f\"{display_name} raw keys:\", list(raw.keys()))\n",
    "    if \"X\" in raw:\n",
    "        df = raw[\"X\"]\n",
    "    else:\n",
    "        # fallback: pick the first pd.DataFrame in raw.values()\n",
    "        dfs = [v for v in raw.values() if isinstance(v, pd.DataFrame)]\n",
    "        if not dfs:\n",
    "            raise ValueError(f\"No DataFrame found in TSDB output for '{tsdb_name}'. Keys: {list(raw.keys())}\")\n",
    "        df = dfs[0]\n",
    "    # ensure datetime index if possible\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df = df.sort_index()\n",
    "    data_dict[display_name] = df\n",
    "    print(f\"  → Loaded {display_name} as DataFrame {df.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd134a",
   "metadata": {},
   "source": [
    "## 4. Simulate missingness\n",
    "\n",
    "- **Random MCAR**: 10% missing uniformly at random.\n",
    "- **Block gap**: a 7-day contiguous gap where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c796a3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AirQuality: rows 7270–8204 set to NaN (missing=10.0%)\n",
      "Electricity: rows 121958–135982 set to NaN (missing=10.0%)\n",
      "Traffic: rows 7270–9023 set to NaN (missing=10.0%)\n",
      "PhysioNet: rows 121958–140012 set to NaN (missing=10.0%)\n"
     ]
    }
   ],
   "source": [
    "# 4. Simulate missingness (contiguous 10% gap)\n",
    "obs = {}\n",
    "mask = {}\n",
    "\n",
    "for name, df in data_dict.items():\n",
    "    # --- only keep numeric cols ---\n",
    "    df_num = df.select_dtypes(include='number')\n",
    "    arr = df_num.values.astype(float)\n",
    "    n, d = arr.shape\n",
    "\n",
    "    # determine gap length = 10% of time‐steps\n",
    "    gap_len = int(np.floor(n * 0.10))\n",
    "    # choose a start index (you can fix or randomize; here we fix seed for reproducibility)\n",
    "    rng = np.random.RandomState(42)\n",
    "    start = rng.randint(0, n - gap_len + 1)\n",
    "\n",
    "    # build the contiguous block mask\n",
    "    m_block = np.zeros_like(arr, dtype=bool)\n",
    "    m_block[start : start + gap_len, :] = True\n",
    "\n",
    "    # apply mask\n",
    "    arr_obs = arr.copy()\n",
    "    arr_obs[m_block] = np.nan\n",
    "\n",
    "    # store\n",
    "    obs[name]  = arr_obs\n",
    "    mask[name] = m_block\n",
    "\n",
    "    print(f\"{name}: rows {start}–{start+gap_len-1} set to NaN (missing={m_block.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667113b",
   "metadata": {},
   "source": [
    "## 5. simulated Annealing Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0070de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_annealing(ts, mask, num_reads=100, prior=None, prior_weight=1.0):\n",
    "    import numpy as _np\n",
    "    from neal import SimulatedAnnealingSampler as Sampler\n",
    "\n",
    "    imp = ts.copy()\n",
    "    idx = _np.where(mask)[0]\n",
    "    if idx.size == 0:\n",
    "        return imp\n",
    "    s, e = idx[0], idx[-1]\n",
    "    y0 = ts[s-1] if s>0 else ts[e+1] if e+1<len(ts) else 0.0\n",
    "    yN = ts[e+1] if e+1<len(ts) else ts[s-1] if s>0 else 0.0\n",
    "    N = e-s+1\n",
    "    d = (yN-y0)/(N+1)\n",
    "    exp = _np.linspace(y0+d, yN-d, N)\n",
    "    known = ts[~mask]\n",
    "    minv, maxv = known.min(), known.max()\n",
    "\n",
    "    Q, var_idx, inv_map, c = {}, {}, {}, 0\n",
    "    for i,v in enumerate(exp):\n",
    "        opts = {int(_np.floor(v)), int(_np.ceil(v))}\n",
    "        if len(opts)==1:\n",
    "            x0=opts.pop(); opts={x0-1,x0,x0+1}\n",
    "        for x in sorted(opts):\n",
    "            var_idx[(i,x)] = c\n",
    "            inv_map[c] = (i,x)\n",
    "            c+=1\n",
    "    P=1e6\n",
    "    def add(a,b,val):\n",
    "        key=(min(a,b),max(a,b))\n",
    "        Q[key]=Q.get(key,0)+val\n",
    "\n",
    "    # (a) seasonality / range penalties\n",
    "    for (i,x),qi in var_idx.items():\n",
    "        add(qi,qi,(x-exp[i])**2)\n",
    "        if x>maxv: add(qi,qi,1e6*(x-maxv)**4)\n",
    "        if x<minv: add(qi,qi,1e4*(minv-x)**2)\n",
    "\n",
    "    # (b) smoothness\n",
    "    for (ii,x1),qi in var_idx.items():\n",
    "        if ii>=N-1: continue\n",
    "        for (jj,x2),qj in var_idx.items():\n",
    "            if jj!=ii+1: continue\n",
    "            add(qi,qj,((x2-x1)-d)**2)\n",
    "\n",
    "    # (c) prior-bias for hybrid (only if provided)\n",
    "    if prior is not None:\n",
    "        for (i,x),qi in var_idx.items():\n",
    "            p_val = prior[s+i]\n",
    "            add(qi,qi, prior_weight*(x-p_val)**2)\n",
    "\n",
    "    # (d) one-hot constraint\n",
    "    for i in range(N):\n",
    "        opts=[x for (ii,x) in var_idx if ii==i]\n",
    "        for a,x1 in enumerate(opts):\n",
    "            qa=var_idx[(i,x1)]\n",
    "            add(qa,qa,-2*P)\n",
    "            for x2 in opts[a+1:]:\n",
    "                qb=var_idx[(i,x2)]\n",
    "                add(qa,qb,2*P)\n",
    "\n",
    "    # Solve\n",
    "    sampler=Sampler()\n",
    "    sampleset=sampler.sample_qubo(Q,num_reads=num_reads)\n",
    "    sol=sampleset.first.sample\n",
    "    for var,bit in sol.items():\n",
    "        if bit:\n",
    "            i,x=inv_map[var]\n",
    "            imp[s+i]=x\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c608f",
   "metadata": {},
   "source": [
    "## 6. SAITS→QA Hybrid Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e4c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_saits_then_anneal(arr, mask, saits_epochs=20, qa_reads=100, prior_weight=1.0):\n",
    "    # first do SAITS\n",
    "    imp_saits = impute_saits(arr.copy(), mask, epochs=saits_epochs)\n",
    "    hybrid = imp_saits.copy()\n",
    "    # then refine ONLY the gap via QA, biasing toward SAITS\n",
    "    for col in range(arr.shape[1]):\n",
    "        hybrid[:,col] = impute_with_annealing(\n",
    "            hybrid[:,col], mask[:,col],\n",
    "            num_reads=qa_reads,\n",
    "            prior=imp_saits[:,col],\n",
    "            prior_weight=prior_weight\n",
    "        )\n",
    "    return hybrid\n",
    "\n",
    "def impute_brits_then_anneal(arr, mask, brits_epochs=20, qa_reads=100, prior_weight=1.0):\n",
    "    # first do BRITS\n",
    "    imp_brits = impute_brits(arr.copy(), mask, epochs=brits_epochs)\n",
    "    hybrid = imp_brits.copy()\n",
    "    # then refine ONLY the gap via QA, biasing toward BRITS\n",
    "    for col in range(arr.shape[1]):\n",
    "        hybrid[:,col] = impute_with_annealing(\n",
    "            hybrid[:,col], mask[:,col],\n",
    "            num_reads=qa_reads,\n",
    "            prior=imp_brits[:,col],\n",
    "            prior_weight=prior_weight\n",
    "        )\n",
    "    return hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7ec4d",
   "metadata": {},
   "source": [
    "## 6b. Baseline Imputers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f48a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_brits(arr, mask, epochs=20):\n",
    "    \"\"\"\n",
    "    BRITS imputer running entirely on CPU.\n",
    "    \"\"\"\n",
    "    arr_masked = arr.copy()\n",
    "    arr_masked[mask] = np.nan\n",
    "    X = torch.tensor(arr_masked[np.newaxis], dtype=torch.float32)\n",
    "    data = {\"X\": X}\n",
    "    model = BRITS(\n",
    "        n_steps=arr.shape[0],\n",
    "        n_features=arr.shape[1],\n",
    "        rnn_hidden_size=64,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    model.fit(data)\n",
    "    imp_t = model.impute(data)    # this is already a numpy.ndarray\n",
    "    return imp_t[0]                # not .numpy()\n",
    "\n",
    "def impute_saits(arr, mask, epochs=20):\n",
    "    \"\"\"\n",
    "    SAITS imputer running entirely on CPU.\n",
    "    \"\"\"\n",
    "    arr_masked = arr.copy()\n",
    "    arr_masked[mask] = np.nan\n",
    "    X = torch.tensor(arr_masked[np.newaxis], dtype=torch.float32)\n",
    "    data = {\"X\": X}\n",
    "    model = SAITS(\n",
    "        n_steps=arr.shape[0],\n",
    "        n_features=arr.shape[1],\n",
    "        n_layers=2,\n",
    "        d_model=64,\n",
    "        n_heads=4,\n",
    "        d_k=16,\n",
    "        d_v=16,\n",
    "        d_ffn=256,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    model.fit(data)\n",
    "    imp_t = model.impute(data)    # already numpy.ndarray\n",
    "    return imp_t[0]                # not .numpy()\n",
    "\n",
    "knn_imp = lambda arr, mask: KNNImputer(n_neighbors=5).fit_transform(\n",
    "    np.where(mask, np.nan, arr)\n",
    ")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "mice_imp = lambda arr, mask: scaler.inverse_transform(\n",
    "    IterativeImputer(\n",
    "        estimator=Ridge(alpha=1.0),\n",
    "        max_iter=20,\n",
    "        tol=1e-3,\n",
    "        initial_strategy='median',\n",
    "        random_state=0\n",
    "    ).fit_transform(\n",
    "        scaler.fit_transform(np.where(mask, np.nan, arr))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620fe3ae",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60b8ef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 11:50:18 [INFO]: No given device, using default device: cpu\n",
      "2025-05-22 11:50:18 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-22 11:50:18 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-22 11:50:18 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-05-22 11:50:18 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 52,016\n",
      "2025-05-22 11:50:28 [INFO]: Epoch 001 - training loss (MAE): 1003.5520\n",
      "2025-05-22 11:50:37 [INFO]: Epoch 002 - training loss (MAE): 998.2177\n",
      "2025-05-22 11:50:46 [INFO]: Epoch 003 - training loss (MAE): 992.9183\n",
      "2025-05-22 11:50:55 [INFO]: Epoch 004 - training loss (MAE): 987.6596\n",
      "2025-05-22 11:51:04 [INFO]: Epoch 005 - training loss (MAE): 982.4376\n",
      "2025-05-22 11:51:13 [INFO]: Epoch 006 - training loss (MAE): 977.2526\n",
      "2025-05-22 11:51:22 [INFO]: Epoch 007 - training loss (MAE): 972.1053\n",
      "2025-05-22 11:51:32 [INFO]: Epoch 008 - training loss (MAE): 966.9975\n",
      "2025-05-22 11:51:41 [INFO]: Epoch 009 - training loss (MAE): 961.9261\n",
      "2025-05-22 11:51:50 [INFO]: Epoch 010 - training loss (MAE): 956.8942\n",
      "2025-05-22 11:51:59 [INFO]: Epoch 011 - training loss (MAE): 951.8998\n",
      "2025-05-22 11:52:09 [INFO]: Epoch 012 - training loss (MAE): 946.9479\n",
      "2025-05-22 11:52:18 [INFO]: Epoch 013 - training loss (MAE): 942.0364\n",
      "2025-05-22 11:52:27 [INFO]: Epoch 014 - training loss (MAE): 937.1686\n",
      "2025-05-22 11:52:36 [INFO]: Epoch 015 - training loss (MAE): 932.3431\n",
      "2025-05-22 11:52:45 [INFO]: Epoch 016 - training loss (MAE): 927.5598\n",
      "2025-05-22 11:52:55 [INFO]: Epoch 017 - training loss (MAE): 922.8226\n",
      "2025-05-22 11:53:04 [INFO]: Epoch 018 - training loss (MAE): 918.1302\n",
      "2025-05-22 11:53:13 [INFO]: Epoch 019 - training loss (MAE): 913.4855\n",
      "2025-05-22 11:53:22 [INFO]: Epoch 020 - training loss (MAE): 908.8845\n",
      "2025-05-22 11:53:31 [INFO]: Epoch 021 - training loss (MAE): 904.3267\n",
      "2025-05-22 11:53:41 [INFO]: Epoch 022 - training loss (MAE): 899.8234\n",
      "2025-05-22 11:53:50 [INFO]: Epoch 023 - training loss (MAE): 895.3651\n",
      "2025-05-22 11:53:59 [INFO]: Epoch 024 - training loss (MAE): 890.9568\n",
      "2025-05-22 11:54:09 [INFO]: Epoch 025 - training loss (MAE): 886.5922\n",
      "2025-05-22 11:54:18 [INFO]: Epoch 026 - training loss (MAE): 882.2775\n",
      "2025-05-22 11:54:27 [INFO]: Epoch 027 - training loss (MAE): 878.0104\n",
      "2025-05-22 11:54:36 [INFO]: Epoch 028 - training loss (MAE): 873.7897\n",
      "2025-05-22 11:54:46 [INFO]: Epoch 029 - training loss (MAE): 869.6152\n",
      "2025-05-22 11:54:55 [INFO]: Epoch 030 - training loss (MAE): 865.4875\n",
      "2025-05-22 11:55:04 [INFO]: Epoch 031 - training loss (MAE): 861.4155\n",
      "2025-05-22 11:55:14 [INFO]: Epoch 032 - training loss (MAE): 857.3853\n",
      "2025-05-22 11:55:23 [INFO]: Epoch 033 - training loss (MAE): 853.4022\n",
      "2025-05-22 11:55:32 [INFO]: Epoch 034 - training loss (MAE): 849.4657\n",
      "2025-05-22 11:55:42 [INFO]: Epoch 035 - training loss (MAE): 845.5724\n",
      "2025-05-22 11:55:51 [INFO]: Epoch 036 - training loss (MAE): 841.7283\n",
      "2025-05-22 11:56:00 [INFO]: Epoch 037 - training loss (MAE): 837.9258\n",
      "2025-05-22 11:56:10 [INFO]: Epoch 038 - training loss (MAE): 834.1653\n",
      "2025-05-22 11:56:19 [INFO]: Epoch 039 - training loss (MAE): 830.4414\n",
      "2025-05-22 11:56:28 [INFO]: Epoch 040 - training loss (MAE): 826.7572\n",
      "2025-05-22 11:56:38 [INFO]: Epoch 041 - training loss (MAE): 823.1072\n",
      "2025-05-22 11:56:47 [INFO]: Epoch 042 - training loss (MAE): 819.4945\n",
      "2025-05-22 11:56:56 [INFO]: Epoch 043 - training loss (MAE): 815.9105\n",
      "2025-05-22 11:57:06 [INFO]: Epoch 044 - training loss (MAE): 812.3597\n",
      "2025-05-22 11:57:15 [INFO]: Epoch 045 - training loss (MAE): 808.8409\n",
      "2025-05-22 11:57:25 [INFO]: Epoch 046 - training loss (MAE): 805.3509\n",
      "2025-05-22 11:57:34 [INFO]: Epoch 047 - training loss (MAE): 801.8839\n",
      "2025-05-22 11:57:43 [INFO]: Epoch 048 - training loss (MAE): 798.4511\n",
      "2025-05-22 11:57:53 [INFO]: Epoch 049 - training loss (MAE): 795.0493\n",
      "2025-05-22 11:58:02 [INFO]: Epoch 050 - training loss (MAE): 791.6732\n",
      "2025-05-22 11:58:12 [INFO]: Epoch 051 - training loss (MAE): 788.3226\n",
      "2025-05-22 11:58:21 [INFO]: Epoch 052 - training loss (MAE): 785.0016\n",
      "2025-05-22 11:58:30 [INFO]: Epoch 053 - training loss (MAE): 781.7108\n",
      "2025-05-22 11:58:40 [INFO]: Epoch 054 - training loss (MAE): 778.4536\n",
      "2025-05-22 11:58:49 [INFO]: Epoch 055 - training loss (MAE): 775.2196\n",
      "2025-05-22 11:58:58 [INFO]: Epoch 056 - training loss (MAE): 772.0203\n",
      "2025-05-22 11:59:08 [INFO]: Epoch 057 - training loss (MAE): 768.8470\n",
      "2025-05-22 11:59:17 [INFO]: Epoch 058 - training loss (MAE): 765.7042\n",
      "2025-05-22 11:59:26 [INFO]: Epoch 059 - training loss (MAE): 762.5908\n",
      "2025-05-22 11:59:36 [INFO]: Epoch 060 - training loss (MAE): 759.5114\n",
      "2025-05-22 11:59:45 [INFO]: Epoch 061 - training loss (MAE): 756.4601\n",
      "2025-05-22 11:59:54 [INFO]: Epoch 062 - training loss (MAE): 753.4394\n",
      "2025-05-22 12:00:04 [INFO]: Epoch 063 - training loss (MAE): 750.4557\n",
      "2025-05-22 12:00:13 [INFO]: Epoch 064 - training loss (MAE): 747.5015\n",
      "2025-05-22 12:00:23 [INFO]: Epoch 065 - training loss (MAE): 744.5829\n",
      "2025-05-22 12:00:32 [INFO]: Epoch 066 - training loss (MAE): 741.6924\n",
      "2025-05-22 12:00:42 [INFO]: Epoch 067 - training loss (MAE): 738.8369\n",
      "2025-05-22 12:00:51 [INFO]: Epoch 068 - training loss (MAE): 736.0182\n",
      "2025-05-22 12:01:00 [INFO]: Epoch 069 - training loss (MAE): 733.2281\n",
      "2025-05-22 12:01:10 [INFO]: Epoch 070 - training loss (MAE): 730.4738\n",
      "2025-05-22 12:01:19 [INFO]: Epoch 071 - training loss (MAE): 727.7523\n",
      "2025-05-22 12:01:28 [INFO]: Epoch 072 - training loss (MAE): 725.0602\n",
      "2025-05-22 12:01:37 [INFO]: Epoch 073 - training loss (MAE): 722.4048\n",
      "2025-05-22 12:01:47 [INFO]: Epoch 074 - training loss (MAE): 719.7780\n",
      "2025-05-22 12:01:56 [INFO]: Epoch 075 - training loss (MAE): 717.1846\n",
      "2025-05-22 12:02:06 [INFO]: Epoch 076 - training loss (MAE): 714.6202\n",
      "2025-05-22 12:02:15 [INFO]: Epoch 077 - training loss (MAE): 712.0872\n",
      "2025-05-22 12:02:24 [INFO]: Epoch 078 - training loss (MAE): 709.5867\n",
      "2025-05-22 12:02:33 [INFO]: Epoch 079 - training loss (MAE): 707.1146\n",
      "2025-05-22 12:02:42 [INFO]: Epoch 080 - training loss (MAE): 704.6726\n",
      "2025-05-22 12:02:52 [INFO]: Epoch 081 - training loss (MAE): 702.2643\n",
      "2025-05-22 12:03:01 [INFO]: Epoch 082 - training loss (MAE): 699.8798\n",
      "2025-05-22 12:03:11 [INFO]: Epoch 083 - training loss (MAE): 697.5298\n",
      "2025-05-22 12:03:20 [INFO]: Epoch 084 - training loss (MAE): 695.2027\n",
      "2025-05-22 12:03:29 [INFO]: Epoch 085 - training loss (MAE): 692.9042\n",
      "2025-05-22 12:03:39 [INFO]: Epoch 086 - training loss (MAE): 690.6328\n",
      "2025-05-22 12:03:48 [INFO]: Epoch 087 - training loss (MAE): 688.3940\n",
      "2025-05-22 12:03:57 [INFO]: Epoch 088 - training loss (MAE): 686.1771\n",
      "2025-05-22 12:04:07 [INFO]: Epoch 089 - training loss (MAE): 683.9839\n",
      "2025-05-22 12:04:16 [INFO]: Epoch 090 - training loss (MAE): 681.8189\n",
      "2025-05-22 12:04:25 [INFO]: Epoch 091 - training loss (MAE): 679.6760\n",
      "2025-05-22 12:04:35 [INFO]: Epoch 092 - training loss (MAE): 677.5529\n",
      "2025-05-22 12:04:44 [INFO]: Epoch 093 - training loss (MAE): 675.4531\n",
      "2025-05-22 12:04:54 [INFO]: Epoch 094 - training loss (MAE): 673.3741\n",
      "2025-05-22 12:05:03 [INFO]: Epoch 095 - training loss (MAE): 671.3121\n",
      "2025-05-22 12:05:12 [INFO]: Epoch 096 - training loss (MAE): 669.2693\n",
      "2025-05-22 12:05:21 [INFO]: Epoch 097 - training loss (MAE): 667.2416\n",
      "2025-05-22 12:05:31 [INFO]: Epoch 098 - training loss (MAE): 665.2301\n",
      "2025-05-22 12:05:41 [INFO]: Epoch 099 - training loss (MAE): 663.2388\n",
      "2025-05-22 12:05:51 [INFO]: Epoch 100 - training loss (MAE): 661.2582\n",
      "2025-05-22 12:06:00 [INFO]: Epoch 101 - training loss (MAE): 659.2927\n",
      "2025-05-22 12:06:09 [INFO]: Epoch 102 - training loss (MAE): 657.3380\n",
      "2025-05-22 12:06:19 [INFO]: Epoch 103 - training loss (MAE): 655.3989\n",
      "2025-05-22 12:06:28 [INFO]: Epoch 104 - training loss (MAE): 653.4711\n",
      "2025-05-22 12:06:38 [INFO]: Epoch 105 - training loss (MAE): 651.5566\n",
      "2025-05-22 12:06:47 [INFO]: Epoch 106 - training loss (MAE): 649.6494\n",
      "2025-05-22 12:06:57 [INFO]: Epoch 107 - training loss (MAE): 647.7480\n",
      "2025-05-22 12:07:06 [INFO]: Epoch 108 - training loss (MAE): 645.8531\n",
      "2025-05-22 12:07:16 [INFO]: Epoch 109 - training loss (MAE): 643.9603\n",
      "2025-05-22 12:07:25 [INFO]: Epoch 110 - training loss (MAE): 642.0733\n",
      "2025-05-22 12:07:35 [INFO]: Epoch 111 - training loss (MAE): 640.1866\n",
      "2025-05-22 12:07:44 [INFO]: Epoch 112 - training loss (MAE): 638.3003\n",
      "2025-05-22 12:07:54 [INFO]: Epoch 113 - training loss (MAE): 636.4134\n",
      "2025-05-22 12:08:03 [INFO]: Epoch 114 - training loss (MAE): 634.5280\n",
      "2025-05-22 12:08:13 [INFO]: Epoch 115 - training loss (MAE): 632.6408\n",
      "2025-05-22 12:08:22 [INFO]: Epoch 116 - training loss (MAE): 630.7520\n",
      "2025-05-22 12:08:31 [INFO]: Epoch 117 - training loss (MAE): 628.8641\n",
      "2025-05-22 12:08:41 [INFO]: Epoch 118 - training loss (MAE): 626.9709\n",
      "2025-05-22 12:08:50 [INFO]: Epoch 119 - training loss (MAE): 625.0800\n",
      "2025-05-22 12:09:00 [INFO]: Epoch 120 - training loss (MAE): 623.1849\n",
      "2025-05-22 12:09:09 [INFO]: Epoch 121 - training loss (MAE): 621.2892\n",
      "2025-05-22 12:09:18 [INFO]: Epoch 122 - training loss (MAE): 619.3930\n",
      "2025-05-22 12:09:28 [INFO]: Epoch 123 - training loss (MAE): 617.4944\n",
      "2025-05-22 12:09:37 [INFO]: Epoch 124 - training loss (MAE): 615.5926\n",
      "2025-05-22 12:09:46 [INFO]: Epoch 125 - training loss (MAE): 613.6940\n",
      "2025-05-22 12:09:56 [INFO]: Epoch 126 - training loss (MAE): 611.7938\n",
      "2025-05-22 12:10:05 [INFO]: Epoch 127 - training loss (MAE): 609.8935\n",
      "2025-05-22 12:10:15 [INFO]: Epoch 128 - training loss (MAE): 607.9905\n",
      "2025-05-22 12:10:24 [INFO]: Epoch 129 - training loss (MAE): 606.0825\n",
      "2025-05-22 12:10:34 [INFO]: Epoch 130 - training loss (MAE): 604.1807\n",
      "2025-05-22 12:10:43 [INFO]: Epoch 131 - training loss (MAE): 602.2825\n",
      "2025-05-22 12:10:53 [INFO]: Epoch 132 - training loss (MAE): 600.3809\n",
      "2025-05-22 12:11:02 [INFO]: Epoch 133 - training loss (MAE): 598.4799\n",
      "2025-05-22 12:11:11 [INFO]: Epoch 134 - training loss (MAE): 596.5785\n",
      "2025-05-22 12:11:21 [INFO]: Epoch 135 - training loss (MAE): 594.6785\n",
      "2025-05-22 12:11:31 [INFO]: Epoch 136 - training loss (MAE): 592.7791\n",
      "2025-05-22 12:11:40 [INFO]: Epoch 137 - training loss (MAE): 590.8824\n",
      "2025-05-22 12:11:49 [INFO]: Epoch 138 - training loss (MAE): 588.9864\n",
      "2025-05-22 12:11:59 [INFO]: Epoch 139 - training loss (MAE): 587.0898\n",
      "2025-05-22 12:12:08 [INFO]: Epoch 140 - training loss (MAE): 585.1952\n",
      "2025-05-22 12:12:18 [INFO]: Epoch 141 - training loss (MAE): 583.3016\n",
      "2025-05-22 12:12:27 [INFO]: Epoch 142 - training loss (MAE): 581.4117\n",
      "2025-05-22 12:12:36 [INFO]: Epoch 143 - training loss (MAE): 579.5210\n",
      "2025-05-22 12:12:46 [INFO]: Epoch 144 - training loss (MAE): 577.6370\n",
      "2025-05-22 12:12:55 [INFO]: Epoch 145 - training loss (MAE): 575.7550\n",
      "2025-05-22 12:13:04 [INFO]: Epoch 146 - training loss (MAE): 573.8771\n",
      "2025-05-22 12:13:14 [INFO]: Epoch 147 - training loss (MAE): 571.9991\n",
      "2025-05-22 12:13:23 [INFO]: Epoch 148 - training loss (MAE): 570.1254\n",
      "2025-05-22 12:13:33 [INFO]: Epoch 149 - training loss (MAE): 568.2523\n",
      "2025-05-22 12:13:42 [INFO]: Epoch 150 - training loss (MAE): 566.3837\n",
      "2025-05-22 12:13:42 [INFO]: Finished training. The best model is from epoch#150.\n",
      "2025-05-22 12:13:46 [INFO]: No given device, using default device: cpu\n",
      "2025-05-22 12:13:46 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-22 12:13:46 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-05-22 12:13:46 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-05-22 12:13:46 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 326,063\n",
      "2025-05-22 12:13:51 [INFO]: Epoch 001 - training loss (MAE): 911.9761\n",
      "2025-05-22 12:13:54 [INFO]: Epoch 002 - training loss (MAE): 912.7667\n",
      "2025-05-22 12:13:58 [INFO]: Epoch 003 - training loss (MAE): 912.6497\n",
      "2025-05-22 12:14:02 [INFO]: Epoch 004 - training loss (MAE): 911.3545\n",
      "2025-05-22 12:14:06 [INFO]: Epoch 005 - training loss (MAE): 910.9657\n",
      "2025-05-22 12:14:09 [INFO]: Epoch 006 - training loss (MAE): 912.4160\n",
      "2025-05-22 12:14:13 [INFO]: Epoch 007 - training loss (MAE): 910.3018\n",
      "2025-05-22 12:14:17 [INFO]: Epoch 008 - training loss (MAE): 913.2495\n",
      "2025-05-22 12:14:21 [INFO]: Epoch 009 - training loss (MAE): 912.8049\n",
      "2025-05-22 12:14:24 [INFO]: Epoch 010 - training loss (MAE): 912.5906\n",
      "2025-05-22 12:14:28 [INFO]: Epoch 011 - training loss (MAE): 909.3896\n",
      "2025-05-22 12:14:32 [INFO]: Epoch 012 - training loss (MAE): 915.8604\n",
      "2025-05-22 12:14:36 [INFO]: Epoch 013 - training loss (MAE): 912.8677\n",
      "2025-05-22 12:14:40 [INFO]: Epoch 014 - training loss (MAE): 911.6198\n",
      "2025-05-22 12:14:44 [INFO]: Epoch 015 - training loss (MAE): 908.7358\n",
      "2025-05-22 12:14:47 [INFO]: Epoch 016 - training loss (MAE): 908.9233\n",
      "2025-05-22 12:14:51 [INFO]: Epoch 017 - training loss (MAE): 913.5121\n",
      "2025-05-22 12:14:55 [INFO]: Epoch 018 - training loss (MAE): 908.1750\n",
      "2025-05-22 12:14:59 [INFO]: Epoch 019 - training loss (MAE): 907.6970\n",
      "2025-05-22 12:15:02 [INFO]: Epoch 020 - training loss (MAE): 908.3100\n",
      "2025-05-22 12:15:06 [INFO]: Epoch 021 - training loss (MAE): 916.1369\n",
      "2025-05-22 12:15:10 [INFO]: Epoch 022 - training loss (MAE): 908.2379\n",
      "2025-05-22 12:15:14 [INFO]: Epoch 023 - training loss (MAE): 912.9668\n",
      "2025-05-22 12:15:17 [INFO]: Epoch 024 - training loss (MAE): 912.5883\n",
      "2025-05-22 12:15:21 [INFO]: Epoch 025 - training loss (MAE): 907.0828\n",
      "2025-05-22 12:15:25 [INFO]: Epoch 026 - training loss (MAE): 912.6646\n",
      "2025-05-22 12:15:29 [INFO]: Epoch 027 - training loss (MAE): 909.4262\n",
      "2025-05-22 12:15:32 [INFO]: Epoch 028 - training loss (MAE): 912.4293\n",
      "2025-05-22 12:15:36 [INFO]: Epoch 029 - training loss (MAE): 910.8452\n",
      "2025-05-22 12:15:40 [INFO]: Epoch 030 - training loss (MAE): 909.2347\n",
      "2025-05-22 12:15:43 [INFO]: Epoch 031 - training loss (MAE): 908.2915\n",
      "2025-05-22 12:15:47 [INFO]: Epoch 032 - training loss (MAE): 906.5115\n",
      "2025-05-22 12:15:51 [INFO]: Epoch 033 - training loss (MAE): 910.8918\n",
      "2025-05-22 12:15:55 [INFO]: Epoch 034 - training loss (MAE): 910.8171\n",
      "2025-05-22 12:15:59 [INFO]: Epoch 035 - training loss (MAE): 910.6344\n",
      "2025-05-22 12:16:02 [INFO]: Epoch 036 - training loss (MAE): 905.9048\n",
      "2025-05-22 12:16:06 [INFO]: Epoch 037 - training loss (MAE): 910.2025\n",
      "2025-05-22 12:16:10 [INFO]: Epoch 038 - training loss (MAE): 909.7185\n",
      "2025-05-22 12:16:14 [INFO]: Epoch 039 - training loss (MAE): 905.8445\n",
      "2025-05-22 12:16:17 [INFO]: Epoch 040 - training loss (MAE): 907.2026\n",
      "2025-05-22 12:16:21 [INFO]: Epoch 041 - training loss (MAE): 911.5366\n",
      "2025-05-22 12:16:25 [INFO]: Epoch 042 - training loss (MAE): 910.8010\n",
      "2025-05-22 12:16:28 [INFO]: Epoch 043 - training loss (MAE): 907.4047\n",
      "2025-05-22 12:16:32 [INFO]: Epoch 044 - training loss (MAE): 908.9185\n",
      "2025-05-22 12:16:36 [INFO]: Epoch 045 - training loss (MAE): 911.9464\n",
      "2025-05-22 12:16:40 [INFO]: Epoch 046 - training loss (MAE): 912.7188\n",
      "2025-05-22 12:16:43 [INFO]: Epoch 047 - training loss (MAE): 906.4398\n",
      "2025-05-22 12:16:47 [INFO]: Epoch 048 - training loss (MAE): 911.3750\n",
      "2025-05-22 12:16:50 [INFO]: Epoch 049 - training loss (MAE): 906.9990\n",
      "2025-05-22 12:16:54 [INFO]: Epoch 050 - training loss (MAE): 905.9515\n",
      "2025-05-22 12:16:57 [INFO]: Epoch 051 - training loss (MAE): 909.6875\n",
      "2025-05-22 12:17:01 [INFO]: Epoch 052 - training loss (MAE): 906.2606\n",
      "2025-05-22 12:17:04 [INFO]: Epoch 053 - training loss (MAE): 904.8984\n",
      "2025-05-22 12:17:08 [INFO]: Epoch 054 - training loss (MAE): 911.4137\n",
      "2025-05-22 12:17:12 [INFO]: Epoch 055 - training loss (MAE): 908.6370\n",
      "2025-05-22 12:17:15 [INFO]: Epoch 056 - training loss (MAE): 902.7620\n",
      "2025-05-22 12:17:19 [INFO]: Epoch 057 - training loss (MAE): 909.1304\n",
      "2025-05-22 12:17:22 [INFO]: Epoch 058 - training loss (MAE): 905.4353\n",
      "2025-05-22 12:17:26 [INFO]: Epoch 059 - training loss (MAE): 903.3652\n",
      "2025-05-22 12:17:29 [INFO]: Epoch 060 - training loss (MAE): 907.5359\n",
      "2025-05-22 12:17:33 [INFO]: Epoch 061 - training loss (MAE): 912.6901\n",
      "2025-05-22 12:17:36 [INFO]: Epoch 062 - training loss (MAE): 907.3070\n",
      "2025-05-22 12:17:40 [INFO]: Epoch 063 - training loss (MAE): 901.1113\n",
      "2025-05-22 12:17:43 [INFO]: Epoch 064 - training loss (MAE): 905.0436\n",
      "2025-05-22 12:17:47 [INFO]: Epoch 065 - training loss (MAE): 908.8165\n",
      "2025-05-22 12:17:50 [INFO]: Epoch 066 - training loss (MAE): 903.1038\n",
      "2025-05-22 12:17:54 [INFO]: Epoch 067 - training loss (MAE): 902.6252\n",
      "2025-05-22 12:17:57 [INFO]: Epoch 068 - training loss (MAE): 903.4607\n",
      "2025-05-22 12:18:01 [INFO]: Epoch 069 - training loss (MAE): 904.9442\n",
      "2025-05-22 12:18:04 [INFO]: Epoch 070 - training loss (MAE): 903.7048\n",
      "2025-05-22 12:18:08 [INFO]: Epoch 071 - training loss (MAE): 903.7106\n",
      "2025-05-22 12:18:11 [INFO]: Epoch 072 - training loss (MAE): 903.7997\n",
      "2025-05-22 12:18:15 [INFO]: Epoch 073 - training loss (MAE): 903.0407\n",
      "2025-05-22 12:18:18 [INFO]: Epoch 074 - training loss (MAE): 902.2395\n",
      "2025-05-22 12:18:22 [INFO]: Epoch 075 - training loss (MAE): 899.2743\n",
      "2025-05-22 12:18:25 [INFO]: Epoch 076 - training loss (MAE): 904.3190\n",
      "2025-05-22 12:18:29 [INFO]: Epoch 077 - training loss (MAE): 904.4633\n",
      "2025-05-22 12:18:32 [INFO]: Epoch 078 - training loss (MAE): 904.9647\n",
      "2025-05-22 12:18:36 [INFO]: Epoch 079 - training loss (MAE): 901.1830\n",
      "2025-05-22 12:18:39 [INFO]: Epoch 080 - training loss (MAE): 903.6700\n",
      "2025-05-22 12:18:43 [INFO]: Epoch 081 - training loss (MAE): 901.6900\n",
      "2025-05-22 12:18:46 [INFO]: Epoch 082 - training loss (MAE): 899.5955\n",
      "2025-05-22 12:18:50 [INFO]: Epoch 083 - training loss (MAE): 900.4288\n",
      "2025-05-22 12:18:53 [INFO]: Epoch 084 - training loss (MAE): 899.1702\n",
      "2025-05-22 12:18:57 [INFO]: Epoch 085 - training loss (MAE): 905.2646\n",
      "2025-05-22 12:19:01 [INFO]: Epoch 086 - training loss (MAE): 901.0372\n",
      "2025-05-22 12:19:04 [INFO]: Epoch 087 - training loss (MAE): 898.0824\n",
      "2025-05-22 12:19:08 [INFO]: Epoch 088 - training loss (MAE): 900.4832\n",
      "2025-05-22 12:19:11 [INFO]: Epoch 089 - training loss (MAE): 902.1028\n",
      "2025-05-22 12:19:15 [INFO]: Epoch 090 - training loss (MAE): 901.6557\n",
      "2025-05-22 12:19:18 [INFO]: Epoch 091 - training loss (MAE): 896.9889\n",
      "2025-05-22 12:19:22 [INFO]: Epoch 092 - training loss (MAE): 898.5699\n",
      "2025-05-22 12:19:25 [INFO]: Epoch 093 - training loss (MAE): 899.7038\n",
      "2025-05-22 12:19:29 [INFO]: Epoch 094 - training loss (MAE): 900.9188\n",
      "2025-05-22 12:19:32 [INFO]: Epoch 095 - training loss (MAE): 896.0920\n",
      "2025-05-22 12:19:36 [INFO]: Epoch 096 - training loss (MAE): 900.1561\n",
      "2025-05-22 12:19:39 [INFO]: Epoch 097 - training loss (MAE): 899.5453\n",
      "2025-05-22 12:19:43 [INFO]: Epoch 098 - training loss (MAE): 895.6271\n",
      "2025-05-22 12:19:46 [INFO]: Epoch 099 - training loss (MAE): 896.8132\n",
      "2025-05-22 12:19:50 [INFO]: Epoch 100 - training loss (MAE): 897.0732\n",
      "2025-05-22 12:19:53 [INFO]: Epoch 101 - training loss (MAE): 897.0906\n",
      "2025-05-22 12:19:57 [INFO]: Epoch 102 - training loss (MAE): 893.7610\n",
      "2025-05-22 12:20:00 [INFO]: Epoch 103 - training loss (MAE): 892.7271\n",
      "2025-05-22 12:20:04 [INFO]: Epoch 104 - training loss (MAE): 896.2335\n",
      "2025-05-22 12:20:07 [INFO]: Epoch 105 - training loss (MAE): 889.7839\n",
      "2025-05-22 12:20:11 [INFO]: Epoch 106 - training loss (MAE): 891.2661\n",
      "2025-05-22 12:20:14 [INFO]: Epoch 107 - training loss (MAE): 896.7100\n",
      "2025-05-22 12:20:18 [INFO]: Epoch 108 - training loss (MAE): 891.9825\n",
      "2025-05-22 12:20:21 [INFO]: Epoch 109 - training loss (MAE): 892.5382\n",
      "2025-05-22 12:20:25 [INFO]: Epoch 110 - training loss (MAE): 896.1047\n",
      "2025-05-22 12:20:28 [INFO]: Epoch 111 - training loss (MAE): 891.9761\n",
      "2025-05-22 12:20:32 [INFO]: Epoch 112 - training loss (MAE): 893.6067\n",
      "2025-05-22 12:20:35 [INFO]: Epoch 113 - training loss (MAE): 889.0529\n",
      "2025-05-22 12:20:38 [INFO]: Epoch 114 - training loss (MAE): 890.4270\n",
      "2025-05-22 12:20:42 [INFO]: Epoch 115 - training loss (MAE): 888.9402\n",
      "2025-05-22 12:20:45 [INFO]: Epoch 116 - training loss (MAE): 889.9771\n",
      "2025-05-22 12:20:49 [INFO]: Epoch 117 - training loss (MAE): 891.1274\n",
      "2025-05-22 12:20:52 [INFO]: Epoch 118 - training loss (MAE): 890.2695\n",
      "2025-05-22 12:20:56 [INFO]: Epoch 119 - training loss (MAE): 890.0124\n",
      "2025-05-22 12:20:59 [INFO]: Epoch 120 - training loss (MAE): 893.8771\n",
      "2025-05-22 12:21:03 [INFO]: Epoch 121 - training loss (MAE): 887.2590\n",
      "2025-05-22 12:21:06 [INFO]: Epoch 122 - training loss (MAE): 887.2132\n",
      "2025-05-22 12:21:10 [INFO]: Epoch 123 - training loss (MAE): 890.8380\n",
      "2025-05-22 12:21:13 [INFO]: Epoch 124 - training loss (MAE): 887.7359\n",
      "2025-05-22 12:21:17 [INFO]: Epoch 125 - training loss (MAE): 883.0073\n",
      "2025-05-22 12:21:20 [INFO]: Epoch 126 - training loss (MAE): 886.3055\n",
      "2025-05-22 12:21:23 [INFO]: Epoch 127 - training loss (MAE): 886.0507\n",
      "2025-05-22 12:21:27 [INFO]: Epoch 128 - training loss (MAE): 883.1890\n",
      "2025-05-22 12:21:30 [INFO]: Epoch 129 - training loss (MAE): 888.2136\n",
      "2025-05-22 12:21:34 [INFO]: Epoch 130 - training loss (MAE): 881.0551\n",
      "2025-05-22 12:21:37 [INFO]: Epoch 131 - training loss (MAE): 879.9855\n",
      "2025-05-22 12:21:41 [INFO]: Epoch 132 - training loss (MAE): 881.8961\n",
      "2025-05-22 12:21:44 [INFO]: Epoch 133 - training loss (MAE): 884.9542\n",
      "2025-05-22 12:21:48 [INFO]: Epoch 134 - training loss (MAE): 879.7627\n",
      "2025-05-22 12:21:51 [INFO]: Epoch 135 - training loss (MAE): 881.4458\n",
      "2025-05-22 12:21:55 [INFO]: Epoch 136 - training loss (MAE): 879.5161\n",
      "2025-05-22 12:21:58 [INFO]: Epoch 137 - training loss (MAE): 880.6492\n",
      "2025-05-22 12:22:02 [INFO]: Epoch 138 - training loss (MAE): 880.7754\n",
      "2025-05-22 12:22:05 [INFO]: Epoch 139 - training loss (MAE): 877.4025\n",
      "2025-05-22 12:22:08 [INFO]: Epoch 140 - training loss (MAE): 879.7421\n",
      "2025-05-22 12:22:12 [INFO]: Epoch 141 - training loss (MAE): 876.8911\n",
      "2025-05-22 12:22:15 [INFO]: Epoch 142 - training loss (MAE): 873.1532\n",
      "2025-05-22 12:22:19 [INFO]: Epoch 143 - training loss (MAE): 875.8024\n",
      "2025-05-22 12:22:22 [INFO]: Epoch 144 - training loss (MAE): 874.7676\n",
      "2025-05-22 12:22:25 [INFO]: Epoch 145 - training loss (MAE): 873.7836\n",
      "2025-05-22 12:22:29 [INFO]: Epoch 146 - training loss (MAE): 878.7222\n",
      "2025-05-22 12:22:32 [INFO]: Epoch 147 - training loss (MAE): 875.6398\n",
      "2025-05-22 12:22:36 [INFO]: Epoch 148 - training loss (MAE): 877.0902\n",
      "2025-05-22 12:22:39 [INFO]: Epoch 149 - training loss (MAE): 870.6520\n",
      "2025-05-22 12:22:42 [INFO]: Epoch 150 - training loss (MAE): 868.9213\n",
      "2025-05-22 12:22:42 [INFO]: Finished training. The best model is from epoch#150.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "5 columns passed, passed data had 6 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Tachyon/lib/python3.13/site-packages/pandas/core/internals/construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Tachyon/lib/python3.13/site-packages/pandas/core/internals/construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 5 columns passed, passed data had 6 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 44\u001b[0m\n\u001b[1;32m     34\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;241m*\u001b[39mmetrics(arr, sa_imp, m,  elapsed_time), name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAITS\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m        # SAITS→QA hybrid\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m        sq_imp = impute_saits_then_anneal(arr, m, saits_epochs=20, qa_reads=100)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m        results.append((*metrics(arr, bq_imp, m), name, 'BRITS+QA'))\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m df_res \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRMSE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAPE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMethod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Tachyon/lib/python3.13/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Tachyon/lib/python3.13/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Tachyon/lib/python3.13/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Tachyon/lib/python3.13/site-packages/pandas/core/internals/construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 5 columns passed, passed data had 6 columns"
     ]
    }
   ],
   "source": [
    "def metrics(true, imp, mask, elapsed_time):\n",
    "    d = imp - true\n",
    "    rmse = np.sqrt(np.nanmean((d[mask])**2))\n",
    "    mae  = np.nanmean(np.abs(d[mask]))\n",
    "    nz   = true[mask] != 0\n",
    "    mape = np.nanmean(np.abs(d[mask][nz] / true[mask][nz])) * 100\n",
    "    return rmse, mae, mape, elapsed_time\n",
    "\n",
    "results = []\n",
    "for name, df in data_dict.items():\n",
    "    if name==\"AirQuality\":\n",
    "        arr = df.select_dtypes(include='number').values.astype(float)\n",
    "        m   = mask[name]\n",
    "        start_time = time.time()\n",
    "        # simulated Annealing only\n",
    "        qa_imp = np.vstack([\n",
    "            impute_with_annealing(arr[:,c], m[:,c])\n",
    "            for c in range(arr.shape[1])\n",
    "        ]).T\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        results.append((*metrics(arr, qa_imp, m,  elapsed_time), name, 'simulatedAnneal'))\n",
    "        # BRITS\n",
    "        start_time = time.time()\n",
    "        br_imp = impute_brits(arr, m, 150)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        results.append((*metrics(arr, br_imp, m, elapsed_time), name, 'BRITS'))\n",
    "        # SAITS\n",
    "        start_time = time.time()\n",
    "        sa_imp = impute_saits(arr, m, 150)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        results.append((*metrics(arr, sa_imp, m,  elapsed_time), name, 'SAITS'))\n",
    "        \"\"\"\n",
    "        # SAITS→QA hybrid\n",
    "        sq_imp = impute_saits_then_anneal(arr, m, saits_epochs=20, qa_reads=100)\n",
    "        results.append((*metrics(arr, sq_imp, m), name, 'SAITS+QA'))\n",
    "        # BRITS→QA hybrid\n",
    "        bq_imp = impute_brits_then_anneal(arr, m, brits_epochs=20, qa_reads=100)\n",
    "        results.append((*metrics(arr, bq_imp, m), name, 'BRITS+QA'))\n",
    "        \"\"\"\n",
    "\n",
    "df_res = pd.DataFrame(\n",
    "    results,\n",
    "    columns=['RMSE','MAE','MAPE','Dataset','Method']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4aa695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Plot RMSE comparison\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=df_res, x='Dataset', y='RMSE', hue='Method')\n",
    "plt.title('RMSE by Method and Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53bdf63c-e96f-4eb4-bd82-4add9e6d56f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float64(294.5503261751803),\n",
       "  np.float64(156.3438307610037),\n",
       "  np.float64(85.08588158095988),\n",
       "  20.947827100753784,\n",
       "  'AirQuality',\n",
       "  'simulatedAnneal'),\n",
       " (np.float64(645.5327736062553),\n",
       "  np.float64(423.10142732354876),\n",
       "  np.float64(97.36174119679492),\n",
       "  1407.4743611812592,\n",
       "  'AirQuality',\n",
       "  'BRITS'),\n",
       " (np.float64(627.9780744529519),\n",
       "  np.float64(405.6969662733688),\n",
       "  np.float64(106.14751045906563),\n",
       "  538.4755589962006,\n",
       "  'AirQuality',\n",
       "  'SAITS')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf6b89-7d56-4221-ac6d-f0fa0a860425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
